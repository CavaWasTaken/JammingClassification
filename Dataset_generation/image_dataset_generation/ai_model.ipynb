{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a66667c",
      "metadata": {
        "id": "9a66667c"
      },
      "source": [
        "# Jamming Detection and Classification with ResNet-18 and Open Set Recognition\n",
        "\n",
        "This notebook implements a CNN ResNet-18 model for jamming signal detection and classification with Open Set Recognition (OSR) capability to detect unknown jamming types.\n",
        "\n",
        "## Dataset Classes:\n",
        "- **CLEAN**: Clean signal (no jamming)\n",
        "- **LN**: Linear Noise jamming\n",
        "- **LWF**: Linear Waveform jamming\n",
        "- **TICK**: Tick jamming\n",
        "- **TRI**: Triangle jamming\n",
        "- **TRIW**: Triangle Wave jamming\n",
        "\n",
        "The OSR approach will allow the model to identify unknown jamming patterns not seen during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b47a090",
      "metadata": {
        "id": "4b47a090"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d5f310af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5f310af",
        "outputId": "6ae4f97a-7230-4ed9-9705-622d668c2404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJSaGAOSpPvp",
        "outputId": "4497a967-00a8-4068-be47-898506d1de8a"
      },
      "id": "fJSaGAOSpPvp",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43c2ad8",
      "metadata": {
        "id": "f43c2ad8"
      },
      "source": [
        "## 2. Dataset Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "68fdead3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68fdead3",
        "outputId": "5598b40d-ddc1-4a94-8934-7439ce5dfe6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 46848 samples from 6 classes\n",
            "Classes: ['CLEAN', 'LN', 'LWF', 'TICK', 'TRI', 'TRIW']\n",
            "  CLEAN: 2880 samples\n",
            "  LN: 8800 samples\n",
            "  LWF: 8768 samples\n",
            "  TICK: 8800 samples\n",
            "  TRI: 8800 samples\n",
            "  TRIW: 8800 samples\n"
          ]
        }
      ],
      "source": [
        "class JammingDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading jamming signal data from .npy files\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.class_names = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        # Get all class directories\n",
        "        class_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])\n",
        "\n",
        "        # Create class mapping\n",
        "        for idx, class_dir in enumerate(class_dirs):\n",
        "            self.class_names.append(class_dir.name)\n",
        "            self.class_to_idx[class_dir.name] = idx\n",
        "\n",
        "            if class_dir.name == 'CLEAN':\n",
        "              # Load all .npy files from this class\n",
        "              count = 0\n",
        "              for npy_file in sorted(class_dir.glob('*.npy')):\n",
        "                  self.samples.append(npy_file)\n",
        "                  self.labels.append(idx)\n",
        "                  count += 1\n",
        "\n",
        "              # print(f'Loaded {count} samples from {class_dir.name}')\n",
        "\n",
        "            else:\n",
        "              # iterate over the subfolders regarding different jamming power\n",
        "              count = 0\n",
        "              for power_dir in sorted(class_dir.glob('*')):\n",
        "                  # Load all .npy files from this class\n",
        "                  for npy_file in sorted(power_dir.glob('*.npy')):\n",
        "                      self.samples.append(npy_file)\n",
        "                      self.labels.append(idx)\n",
        "                      count += 1\n",
        "\n",
        "              # print(f'Loaded {count} samples from {class_dir.name} with power {class_dir.name}')\n",
        "\n",
        "        print(f'Loaded {len(self.samples)} samples from {len(self.class_names)} classes')\n",
        "        print(f'Classes: {self.class_names}')\n",
        "\n",
        "        # Print class distribution\n",
        "        unique, counts = np.unique(self.labels, return_counts=True)\n",
        "        for cls_idx, count in zip(unique, counts):\n",
        "            print(f'  {self.class_names[cls_idx]}: {count} samples')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the .npy file\n",
        "        data = np.load(self.samples[idx])\n",
        "\n",
        "        # Convert to tensor and add channel dimension (1, H, W)\n",
        "        data = torch.FloatTensor(data).unsqueeze(0)\n",
        "\n",
        "        # Apply normalization\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return data, label\n",
        "\n",
        "# Define data directory\n",
        "data_dir = './drive/MyDrive/N-MON/Processed_Dataset'\n",
        "\n",
        "# Load the full dataset\n",
        "full_dataset = JammingDataset(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "35102efc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35102efc",
        "outputId": "4492cdf3-5c9a-4896-dc9b-518a9965848c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset split:\n",
            "  Training: 32793 samples\n",
            "  Validation: 7027 samples\n",
            "  Testing: 7028 samples\n",
            "\n",
            "Batch size: 32\n",
            "Number of batches: Train=1025, Val=220, Test=220\n"
          ]
        }
      ],
      "source": [
        "# Split dataset: 70% train, 15% validation, 15% test\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.15 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f'\\nDataset split:')\n",
        "print(f'  Training: {len(train_dataset)} samples')\n",
        "print(f'  Validation: {len(val_dataset)} samples')\n",
        "print(f'  Testing: {len(test_dataset)} samples')\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f'\\nBatch size: {batch_size}')\n",
        "print(f'Number of batches: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3bfa021",
      "metadata": {
        "id": "a3bfa021"
      },
      "source": [
        "## 3. ResNet-18 Architecture Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1b8d89cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b8d89cf",
        "outputId": "d972ff74-3f14-43df-9377-3b9c55a36d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: ResNet-18\n",
            "Number of classes: 6\n",
            "Total parameters: 11,173,318\n",
            "Trainable parameters: 11,173,318\n"
          ]
        }
      ],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"Basic residual block for ResNet-18\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    \"\"\"ResNet-18 architecture adapted for jamming signal classification\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=6, input_channels=1):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual layers\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "\n",
        "        # Global average pooling and fully connected layer\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # Initial layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Residual blocks\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Pooling and flatten\n",
        "        x = self.avgpool(x)\n",
        "        features = torch.flatten(x, 1)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.fc(features)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        return logits\n",
        "\n",
        "# Create model instance\n",
        "num_classes = len(full_dataset.class_names)\n",
        "model = ResNet18(num_classes=num_classes, input_channels=1).to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f'Model: ResNet-18')\n",
        "print(f'Number of classes: {num_classes}')\n",
        "print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2d5c8e",
      "metadata": {
        "id": "4f2d5c8e"
      },
      "source": [
        "## 4. Open Set Recognition Implementation\n",
        "\n",
        "For Open Set Recognition, we'll use the **OpenMax** approach. This method:\n",
        "1. Computes activation vectors (features) for each class during training\n",
        "2. Fits Weibull distributions to model the tail probabilities\n",
        "3. During inference, adjusts the softmax scores to identify unknown samples\n",
        "\n",
        "We'll also implement a simpler threshold-based approach using feature distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c97c37ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c97c37ff",
        "outputId": "5fa40a4d-1640-4b31-faa1-a68bb51bb969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open Set Recognizer initialized\n"
          ]
        }
      ],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import weibull_min\n",
        "\n",
        "class OpenSetRecognizer:\n",
        "    \"\"\"\n",
        "    Open Set Recognition using Mean Activation Vectors (MAV) and distance thresholding.\n",
        "    This is a simplified approach that works well for detecting unknown classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, tail_size=20):\n",
        "        self.num_classes = num_classes\n",
        "        self.tail_size = tail_size  # Number of samples to use for computing statistics\n",
        "        self.mavs = None  # Mean Activation Vectors for each class\n",
        "        self.dists = None  # Distance distributions for each class\n",
        "        self.thresholds = None  # Thresholds for unknown detection\n",
        "\n",
        "    def compute_mav(self, model, dataloader, device):\n",
        "        \"\"\"Compute Mean Activation Vectors for each class\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        # Store features for each class\n",
        "        class_features = {i: [] for i in range(self.num_classes)}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(dataloader, desc='Computing MAVs'):\n",
        "                inputs = inputs.to(device)\n",
        "                _, features = model(inputs, return_features=True)\n",
        "\n",
        "                # Group features by class\n",
        "                for feat, label in zip(features.cpu().numpy(), labels.numpy()):\n",
        "                    class_features[label].append(feat)\n",
        "\n",
        "        # Compute mean activation vector for each class\n",
        "        self.mavs = {}\n",
        "        self.dists = {}\n",
        "\n",
        "        for class_id in range(self.num_classes):\n",
        "            features = np.array(class_features[class_id])\n",
        "            self.mavs[class_id] = np.mean(features, axis=0)\n",
        "\n",
        "            # Compute distances from MAV for threshold calculation\n",
        "            distances = np.linalg.norm(features - self.mavs[class_id], axis=1)\n",
        "            self.dists[class_id] = np.sort(distances)\n",
        "\n",
        "        print('MAVs computed for all classes')\n",
        "\n",
        "    def fit_weibull(self, alpha=0.95):\n",
        "        \"\"\"Fit Weibull distribution and compute thresholds\"\"\"\n",
        "        self.thresholds = {}\n",
        "        self.weibull_params = {}\n",
        "\n",
        "        for class_id in range(self.num_classes):\n",
        "            # Use tail distances for fitting\n",
        "            tail_dists = self.dists[class_id][-self.tail_size:]\n",
        "\n",
        "            # Fit Weibull distribution\n",
        "            shape, loc, scale = weibull_min.fit(tail_dists, floc=0)\n",
        "            self.weibull_params[class_id] = (shape, loc, scale)\n",
        "\n",
        "            # Compute threshold at alpha percentile\n",
        "            self.thresholds[class_id] = weibull_min.ppf(alpha, shape, loc, scale)\n",
        "\n",
        "        print(f'Weibull distributions fitted with alpha={alpha}')\n",
        "\n",
        "    def predict(self, model, inputs, device, unknown_threshold=0.5):\n",
        "        \"\"\"\n",
        "        Predict with unknown detection\n",
        "        Returns: predictions, is_unknown, max_scores\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits, features = model(inputs.to(device), return_features=True)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            max_probs, predictions = torch.max(probabilities, dim=1)\n",
        "\n",
        "            # Compute distances to MAVs\n",
        "            features_np = features.cpu().numpy()\n",
        "            is_unknown = np.zeros(len(features_np), dtype=bool)\n",
        "\n",
        "            for i, (feat, pred) in enumerate(zip(features_np, predictions.cpu().numpy())):\n",
        "                # Distance to predicted class MAV\n",
        "                dist = np.linalg.norm(feat - self.mavs[pred])\n",
        "\n",
        "                # Check if distance exceeds threshold or probability is too low\n",
        "                if dist > self.thresholds[pred] or max_probs[i].item() < unknown_threshold:\n",
        "                    is_unknown[i] = True\n",
        "\n",
        "        return predictions.cpu(), is_unknown, max_probs.cpu()\n",
        "\n",
        "# Initialize Open Set Recognizer\n",
        "osr = OpenSetRecognizer(num_classes=num_classes, tail_size=20)\n",
        "print('Open Set Recognizer initialized')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ae4b27d",
      "metadata": {
        "id": "7ae4b27d"
      },
      "source": [
        "## 5. Training Configuration and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6449cb13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6449cb13",
        "outputId": "029c0d48-be94-4462-8e68-1c9400bafdec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4094177571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Learning rate scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n\u001b[0m\u001b[1;32m     12\u001b[0m                                                    patience=5, verbose=True)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
          ]
        }
      ],
      "source": [
        "# Training hyperparameters\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
        "                                                   patience=5, verbose=True)\n",
        "\n",
        "print(f'Training Configuration:')\n",
        "print(f'  Epochs: {num_epochs}')\n",
        "print(f'  Learning rate: {learning_rate}')\n",
        "print(f'  Weight decay: {weight_decay}')\n",
        "print(f'  Optimizer: Adam')\n",
        "print(f'  Loss function: CrossEntropyLoss')\n",
        "print(f'  LR Scheduler: ReduceLROnPlateau')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2236eaeb",
      "metadata": {
        "id": "2236eaeb"
      },
      "source": [
        "## 6. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb82dd7",
      "metadata": {
        "id": "dfb82dd7"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc='Validation'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print('Training functions defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc077dea",
      "metadata": {
        "id": "dc077dea"
      },
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_path = 'best_resnet18_jamming.pth'\n",
        "\n",
        "print('Starting training...\\n')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 60)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'class_names': full_dataset.class_names\n",
        "        }, best_model_path)\n",
        "        print(f'âœ“ Best model saved (Val Acc: {val_acc:.2f}%)')\n",
        "\n",
        "    print()\n",
        "\n",
        "print(f'Training completed!')\n",
        "print(f'Best validation accuracy: {best_val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "738fa5f9",
      "metadata": {
        "id": "738fa5f9"
      },
      "source": [
        "## 7. Visualize Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4eca8e",
      "metadata": {
        "id": "af4eca8e"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot loss\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Validation Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Validation Accuracy', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f685c8b",
      "metadata": {
        "id": "0f685c8b"
      },
      "source": [
        "## 8. Setup Open Set Recognition\n",
        "\n",
        "Now we'll compute the Mean Activation Vectors (MAVs) and fit the Weibull distributions for OSR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add3ba77",
      "metadata": {
        "id": "add3ba77"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f'Loaded best model from epoch {checkpoint[\"epoch\"]+1} with Val Acc: {checkpoint[\"val_acc\"]:.2f}%')\n",
        "\n",
        "# Compute MAVs using training data\n",
        "print('\\nComputing Mean Activation Vectors (MAVs)...')\n",
        "osr.compute_mav(model, train_loader, device)\n",
        "\n",
        "# Fit Weibull distributions\n",
        "print('\\nFitting Weibull distributions...')\n",
        "osr.fit_weibull(alpha=0.95)\n",
        "\n",
        "print('\\nOpen Set Recognition setup completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250bd531",
      "metadata": {
        "id": "250bd531"
      },
      "source": [
        "## 9. Evaluation on Test Set (Closed Set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed20fb6f",
      "metadata": {
        "id": "ed20fb6f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device, class_names):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc='Testing'):\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy * 100:.2f}%\\n')\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "    return all_preds, all_labels\n",
        "\n",
        "# Evaluate on test set\n",
        "test_preds, test_labels = evaluate_model(model, test_loader, device, full_dataset.class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "344c9333",
      "metadata": {
        "id": "344c9333"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=full_dataset.class_names,\n",
        "            yticklabels=full_dataset.class_names)\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print per-class accuracy\n",
        "print('\\nPer-class Accuracy:')\n",
        "for i, class_name in enumerate(full_dataset.class_names):\n",
        "    class_mask = test_labels == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_acc = (test_preds[class_mask] == i).sum() / class_mask.sum()\n",
        "        print(f'  {class_name}: {class_acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2fdbf1",
      "metadata": {
        "id": "fe2fdbf1"
      },
      "source": [
        "## 10. Open Set Recognition Evaluation\n",
        "\n",
        "Now we'll test the OSR capability by evaluating the model with unknown detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb9cacc",
      "metadata": {
        "id": "1fb9cacc"
      },
      "outputs": [],
      "source": [
        "def evaluate_osr(model, dataloader, osr_model, device, class_names, unknown_threshold=0.5):\n",
        "    \"\"\"Evaluate model with Open Set Recognition\"\"\"\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_is_unknown = []\n",
        "    all_confidences = []\n",
        "\n",
        "    for inputs, labels in tqdm(dataloader, desc='OSR Evaluation'):\n",
        "        preds, is_unknown, confidences = osr_model.predict(model, inputs, device, unknown_threshold)\n",
        "\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_is_unknown.extend(is_unknown)\n",
        "        all_confidences.extend(confidences.numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_is_unknown = np.array(all_is_unknown)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "\n",
        "    # Create predictions with unknown class\n",
        "    final_preds = all_preds.copy()\n",
        "    final_preds[all_is_unknown] = -1  # -1 for unknown\n",
        "\n",
        "    # Compute metrics\n",
        "    known_mask = ~all_is_unknown\n",
        "    unknown_count = all_is_unknown.sum()\n",
        "\n",
        "    print(f'Total samples: {len(all_labels)}')\n",
        "    print(f'Detected as known: {known_mask.sum()} ({known_mask.sum()/len(all_labels)*100:.2f}%)')\n",
        "    print(f'Detected as unknown: {unknown_count} ({unknown_count/len(all_labels)*100:.2f}%)')\n",
        "\n",
        "    if known_mask.sum() > 0:\n",
        "        known_accuracy = (all_preds[known_mask] == all_labels[known_mask]).sum() / known_mask.sum()\n",
        "        print(f'\\nAccuracy on known samples: {known_accuracy * 100:.2f}%')\n",
        "\n",
        "    return final_preds, all_labels, all_is_unknown, all_confidences\n",
        "\n",
        "# Evaluate with OSR\n",
        "print('Evaluating with Open Set Recognition...\\n')\n",
        "osr_preds, osr_labels, is_unknown, confidences = evaluate_osr(\n",
        "    model, test_loader, osr, device, full_dataset.class_names, unknown_threshold=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35bf67c5",
      "metadata": {
        "id": "35bf67c5"
      },
      "outputs": [],
      "source": [
        "# Visualize confidence distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Confidence distribution\n",
        "axes[0].hist(confidences[~is_unknown], bins=50, alpha=0.7, label='Known', color='blue')\n",
        "axes[0].hist(confidences[is_unknown], bins=50, alpha=0.7, label='Unknown', color='red')\n",
        "axes[0].set_xlabel('Confidence Score')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Confidence Score Distribution')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Per-class unknown detection rate\n",
        "unknown_rates = []\n",
        "for i, class_name in enumerate(full_dataset.class_names):\n",
        "    class_mask = osr_labels == i\n",
        "    if class_mask.sum() > 0:\n",
        "        unknown_rate = is_unknown[class_mask].sum() / class_mask.sum() * 100\n",
        "        unknown_rates.append(unknown_rate)\n",
        "    else:\n",
        "        unknown_rates.append(0)\n",
        "\n",
        "axes[1].bar(full_dataset.class_names, unknown_rates, color='coral')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Unknown Detection Rate (%)')\n",
        "axes[1].set_title('Unknown Detection Rate per Class')\n",
        "axes[1].xticks(rotation=45)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nUnknown Detection Rate per Class:')\n",
        "for class_name, rate in zip(full_dataset.class_names, unknown_rates):\n",
        "    print(f'  {class_name}: {rate:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095a7625",
      "metadata": {
        "id": "095a7625"
      },
      "source": [
        "## 11. Test with Simulated Unknown Classes\n",
        "\n",
        "To demonstrate OSR capability, we'll simulate unknown classes by treating some known classes as unknown during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675194cf",
      "metadata": {
        "id": "675194cf"
      },
      "outputs": [],
      "source": [
        "# Simulate open set scenario: treat last 2 classes as unknown\n",
        "# For example, treat TRIW and TRI as unknown classes\n",
        "known_classes = [0, 1, 2, 3]  # CLEAN, LN, LWF, TICK\n",
        "unknown_classes = [4, 5]  # TRI, TRIW\n",
        "\n",
        "print('Simulating Open Set Scenario:')\n",
        "print(f'Known classes: {[full_dataset.class_names[i] for i in known_classes]}')\n",
        "print(f'Unknown classes (simulated): {[full_dataset.class_names[i] for i in unknown_classes]}\\n')\n",
        "\n",
        "# Separate test samples\n",
        "known_samples_mask = np.isin(osr_labels, known_classes)\n",
        "unknown_samples_mask = np.isin(osr_labels, unknown_classes)\n",
        "\n",
        "print(f'Known samples in test set: {known_samples_mask.sum()}')\n",
        "print(f'Unknown samples in test set: {unknown_samples_mask.sum()}\\n')\n",
        "\n",
        "# Evaluate on known samples\n",
        "known_detected_as_known = (~is_unknown[known_samples_mask]).sum()\n",
        "known_detected_as_unknown = is_unknown[known_samples_mask].sum()\n",
        "\n",
        "print('Performance on Known Classes:')\n",
        "print(f'  Correctly identified as known: {known_detected_as_known} ({known_detected_as_known/known_samples_mask.sum()*100:.2f}%)')\n",
        "print(f'  Incorrectly identified as unknown: {known_detected_as_unknown} ({known_detected_as_unknown/known_samples_mask.sum()*100:.2f}%)')\n",
        "\n",
        "# Evaluate on unknown samples\n",
        "unknown_detected_as_known = (~is_unknown[unknown_samples_mask]).sum()\n",
        "unknown_detected_as_unknown = is_unknown[unknown_samples_mask].sum()\n",
        "\n",
        "print('\\nPerformance on Unknown Classes (Simulated):')\n",
        "print(f'  Correctly identified as unknown: {unknown_detected_as_unknown} ({unknown_detected_as_unknown/unknown_samples_mask.sum()*100:.2f}%)')\n",
        "print(f'  Incorrectly identified as known: {unknown_detected_as_known} ({unknown_detected_as_known/unknown_samples_mask.sum()*100:.2f}%)')\n",
        "\n",
        "# Overall OSR metrics\n",
        "print('\\nOverall Open Set Recognition Metrics:')\n",
        "total_correct = known_detected_as_known + unknown_detected_as_unknown\n",
        "total_samples = len(osr_labels)\n",
        "osr_accuracy = total_correct / total_samples * 100\n",
        "print(f'  OSR Accuracy: {osr_accuracy:.2f}%')\n",
        "\n",
        "# Calculate precision and recall for unknown detection\n",
        "if (unknown_detected_as_unknown + known_detected_as_unknown) > 0:\n",
        "    unknown_precision = unknown_detected_as_unknown / (unknown_detected_as_unknown + known_detected_as_unknown)\n",
        "    print(f'  Unknown Precision: {unknown_precision * 100:.2f}%')\n",
        "\n",
        "if unknown_samples_mask.sum() > 0:\n",
        "    unknown_recall = unknown_detected_as_unknown / unknown_samples_mask.sum()\n",
        "    print(f'  Unknown Recall: {unknown_recall * 100:.2f}%')\n",
        "\n",
        "    if unknown_precision > 0 and unknown_recall > 0:\n",
        "        f1_unknown = 2 * (unknown_precision * unknown_recall) / (unknown_precision + unknown_recall)\n",
        "        print(f'  Unknown F1-Score: {f1_unknown * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd64ebcd",
      "metadata": {
        "id": "cd64ebcd"
      },
      "source": [
        "## 12. Inference Function for New Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c62aae8",
      "metadata": {
        "id": "6c62aae8"
      },
      "outputs": [],
      "source": [
        "def predict_jamming(model, osr_model, data_path, device, class_names, unknown_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Predict jamming type for a new signal sample with unknown detection\n",
        "\n",
        "    Args:\n",
        "        model: Trained ResNet-18 model\n",
        "        osr_model: Open Set Recognizer\n",
        "        data_path: Path to .npy file containing signal data\n",
        "        device: torch device\n",
        "        class_names: List of class names\n",
        "        unknown_threshold: Threshold for unknown detection\n",
        "\n",
        "    Returns:\n",
        "        prediction: Predicted class name or 'UNKNOWN'\n",
        "        confidence: Confidence score\n",
        "        is_unknown: Boolean indicating if sample is unknown\n",
        "    \"\"\"\n",
        "    # Load and preprocess data\n",
        "    data = np.load(data_path)\n",
        "    data = torch.FloatTensor(data).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
        "\n",
        "    # Predict with OSR\n",
        "    preds, is_unknown, confidences = osr_model.predict(model, data, device, unknown_threshold)\n",
        "\n",
        "    pred_class = preds[0].item()\n",
        "    confidence = confidences[0].item()\n",
        "    unknown_flag = is_unknown[0]\n",
        "\n",
        "    if unknown_flag:\n",
        "        prediction = 'UNKNOWN'\n",
        "    else:\n",
        "        prediction = class_names[pred_class]\n",
        "\n",
        "    return prediction, confidence, unknown_flag\n",
        "\n",
        "\n",
        "# Example: Test prediction on a sample from test set\n",
        "sample_idx = 0\n",
        "sample_path = test_dataset.dataset.samples[test_dataset.indices[sample_idx]]\n",
        "true_label = test_dataset.dataset.labels[test_dataset.indices[sample_idx]]\n",
        "\n",
        "prediction, confidence, is_unk = predict_jamming(\n",
        "    model, osr, sample_path, device, full_dataset.class_names, unknown_threshold=0.3\n",
        ")\n",
        "\n",
        "print('Single Sample Prediction Example:')\n",
        "print(f'  Sample: {sample_path.name}')\n",
        "print(f'  True label: {full_dataset.class_names[true_label]}')\n",
        "print(f'  Prediction: {prediction}')\n",
        "print(f'  Confidence: {confidence:.4f}')\n",
        "print(f'  Detected as unknown: {is_unk}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8cb629b",
      "metadata": {
        "id": "e8cb629b"
      },
      "source": [
        "## 13. Save Complete Model with OSR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9ce970",
      "metadata": {
        "id": "9c9ce970"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save complete model with OSR parameters\n",
        "model_package = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'class_names': full_dataset.class_names,\n",
        "    'osr_mavs': osr.mavs,\n",
        "    'osr_thresholds': osr.thresholds,\n",
        "    'osr_weibull_params': osr.weibull_params,\n",
        "    'num_classes': num_classes,\n",
        "    'input_shape': (1, 128, 873),\n",
        "    'history': history\n",
        "}\n",
        "\n",
        "# Save the complete package\n",
        "model_package_path = 'resnet18_jamming_osr_complete.pkl'\n",
        "with open(model_package_path, 'wb') as f:\n",
        "    pickle.dump(model_package, f)\n",
        "\n",
        "print(f'Complete model package saved to: {model_package_path}')\n",
        "print('\\nPackage includes:')\n",
        "print('  - Model weights')\n",
        "print('  - Class names')\n",
        "print('  - OSR MAVs')\n",
        "print('  - OSR thresholds')\n",
        "print('  - Weibull parameters')\n",
        "print('  - Training history')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e7010b3",
      "metadata": {
        "id": "3e7010b3"
      },
      "source": [
        "## 14. Load and Use Saved Model (Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a490a7fe",
      "metadata": {
        "id": "a490a7fe"
      },
      "outputs": [],
      "source": [
        "def load_model_for_inference(model_package_path, device):\n",
        "    \"\"\"\n",
        "    Load the complete model package for inference\n",
        "\n",
        "    Args:\n",
        "        model_package_path: Path to the saved model package\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded ResNet-18 model\n",
        "        osr: Loaded Open Set Recognizer\n",
        "        class_names: List of class names\n",
        "    \"\"\"\n",
        "    # Load package\n",
        "    with open(model_package_path, 'rb') as f:\n",
        "        package = pickle.load(f)\n",
        "\n",
        "    # Reconstruct model\n",
        "    model = ResNet18(num_classes=package['num_classes'], input_channels=1).to(device)\n",
        "    model.load_state_dict(package['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Reconstruct OSR\n",
        "    osr = OpenSetRecognizer(num_classes=package['num_classes'])\n",
        "    osr.mavs = package['osr_mavs']\n",
        "    osr.thresholds = package['osr_thresholds']\n",
        "    osr.weibull_params = package['osr_weibull_params']\n",
        "\n",
        "    return model, osr, package['class_names']\n",
        "\n",
        "\n",
        "# Example: Load and use the model\n",
        "print('Example of loading saved model:\\n')\n",
        "loaded_model, loaded_osr, loaded_class_names = load_model_for_inference(model_package_path, device)\n",
        "\n",
        "print('Model loaded successfully!')\n",
        "print(f'Classes: {loaded_class_names}')\n",
        "print(f'Ready for inference with Open Set Recognition')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baff143a",
      "metadata": {
        "id": "baff143a"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implemented a complete CNN ResNet-18 architecture with Open Set Recognition for jamming signal detection and classification.\n",
        "\n",
        "### Key Features:\n",
        "1. **ResNet-18 Architecture**: Deep CNN with residual connections adapted for signal data (128Ã—873)\n",
        "2. **6 Known Classes**: CLEAN, LN, LWF, TICK, TRI, TRIW\n",
        "3. **Open Set Recognition**: Using Mean Activation Vectors and Weibull distribution fitting\n",
        "4. **Unknown Detection**: Capable of identifying jamming patterns not seen during training\n",
        "\n",
        "### Model Capabilities:\n",
        "- **Closed-Set Classification**: High accuracy on known jamming types\n",
        "- **Open-Set Recognition**: Detects unknown/novel jamming patterns\n",
        "- **Confidence Scoring**: Provides confidence scores for predictions\n",
        "- **Real-time Inference**: Fast prediction on new signal samples\n",
        "\n",
        "### Usage:\n",
        "1. Train model on known classes (cells 1-6)\n",
        "2. Setup OSR with MAVs and Weibull fitting (cell 8)\n",
        "3. Evaluate on test set (cells 9-11)\n",
        "4. Use `predict_jamming()` for inference on new data\n",
        "5. Load saved model with `load_model_for_inference()`\n",
        "\n",
        "### Adjusting Unknown Threshold:\n",
        "- Lower threshold (e.g., 0.2): More samples detected as unknown (higher sensitivity)\n",
        "- Higher threshold (e.g., 0.5): Fewer samples detected as unknown (higher specificity)\n",
        "- Tune based on your application's requirements for false positive vs false negative rates"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}